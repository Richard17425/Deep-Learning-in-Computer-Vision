{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "In this portion of the Problem Set, you will gain experience building a CNN for image\n",
    "classification on the MNIST dataset -- a true rite of passage for deep learning and computer\n",
    "vision! Your code should follow this general outline:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the data. Implement a data loader class to load the dataset. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#PyTorch - Importing the Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision               #主要用来构建计算机视觉模型\n",
    "import torchvision.transforms    #用来对图像进行变换(裁切，旋转之类)\n",
    "\n",
    "\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./traindata/', train=True, transform=transforms, download=True)  \n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./testdata/',train=False, transform=transforms, download=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the Data\n",
    "def imshowPytorch(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # H x W x color_channel\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=False)\n",
    "                                           \n",
    "data_iter = iter(train_loader)\n",
    "images, label = next(data_iter)  # images are 32 x 1 x 28 x 28\n",
    "imshowPytorch(torchvision.utils.make_grid(images[10]))\n",
    "print(label[10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the model. We suggest you start with 3 convolutional layers with 24, 48, 64 filters/feature maps, respectively. Each convolutional layer will have 5x5 filters with 2x2 pooling and ReLu activation. Finally, use a fully connected layer to map features to output. (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch - Building the Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_of_class):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        self.fc_model = nn.Sequential(\n",
    "            nn.Linear(400,120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)  # BS x 16 x 5 x 5\n",
    "        x = x.view(-1, 16*5*5)  # 16 x 5 x 5 --> 400 x 1\n",
    "        x = self.fc_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (cnn_model): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (fc_model): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (classifier): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyTorch - Visualizing the Model\n",
    "modelpy = NeuralNet(10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(modelpy.parameters())\n",
    "\n",
    "modelpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utilize a gradient-based optimizer. (5 points)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model! (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.6197645392179489\n",
      "Epoch 1: Loss: 0.42771875548760097\n",
      "Epoch 2: Loss: 0.36543289934396744\n",
      "Epoch 3: Loss: 0.3275402896543344\n",
      "Epoch 4: Loss: 0.30232370724678037\n"
     ]
    }
   ],
   "source": [
    "#PyTorch - Training the Model\n",
    "for e in range(5):\n",
    "    # define the loss value after the epoch\n",
    "    losss = 0.0\n",
    "    number_of_sub_epoch = 0\n",
    "    \n",
    "    # loop for every training batch (one epoch)\n",
    "    for images, labels in train_loader:\n",
    "        #create the output from the network\n",
    "        out = modelpy(images)\n",
    "        # count the loss function\n",
    "        loss = criterion(out, labels)\n",
    "        # in pytorch you have assign the zero for gradien in any sub epoch\n",
    "        optim.zero_grad()\n",
    "        # count the backpropagation\n",
    "        loss.backward()\n",
    "        # learning\n",
    "        optim.step()\n",
    "        # add new value to the main loss\n",
    "        losss += loss.item()\n",
    "        number_of_sub_epoch += 1\n",
    "    print(\"Epoch {}: Loss: {}\".format(e, losss / number_of_sub_epoch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the trained model on an independent test set. What was its performance? What accuracy did your model achieve, and anything in particular that it had trouble with? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 87% with PyTorch\n"
     ]
    }
   ],
   "source": [
    "#PyTorch - Comparing the Results\n",
    "correct = 0\n",
    "total = 0\n",
    "modelpy.eval()  # all of your layers are in eval stage, important for layers such as batchnorm or dropout --> will mess up the result if the network has batchnorm or dropout layers, e.g. RESNET\n",
    "\n",
    "with torch.no_grad(): # all of computation graphs will not update gradients because we will not do any backpropagation --> speed up, doesn't hurt if not called\n",
    "  for images, labels in test_loader:\n",
    "      outputs = modelpy(images)\n",
    "      _, predicted = torch.max(outputs.data, 1)  # outputs.data in shape of BS x 10  -->  BS x 1\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum()\n",
    "print('Test Accuracy of the model on the {} test images: {}% with PyTorch'.format(total, 100 * correct // total))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Provide a summary of any architectural modifications made, plots of the loss evolution from (4), and plots of your results from (5). Please save your code and results for submission. (20 points)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
